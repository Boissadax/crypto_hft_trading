"""
Model Performance Visualizer

Creates comprehensive visualizations for model training results:
- Performance comparison charts (accuracy, precision, recall)
- Confusion matrices for classification models
- ROC curves and AUC analysis
- Training vs validation curves
- Model comparison radar charts
- Ensemble weights visualization
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('default')
sns.set_palette("husl")

class ModelPerformanceVisualizer:
    """Visualize model training and performance results."""
    
    def __init__(self, figsize: Tuple[int, int] = (12, 8), dpi: int = 100):
        """
        Initialize visualizer.
        
        Args:
            figsize: Default figure size
            dpi: Figure DPI for quality
        """
        self.figsize = figsize
        self.dpi = dpi
        self.colors = sns.color_palette("husl", n_colors=10)
        
    def plot_model_comparison(self, 
                            results: Dict[str, Any],
                            metrics: List[str] = None,
                            save_path: Optional[str] = None) -> plt.Figure:
        """
        Create comprehensive model comparison visualization.
        
        Args:
            results: Dictionary of model results from ModelTrainer
            metrics: Metrics to include in comparison
            save_path: Path to save the figure
            
        Returns:
            Matplotlib figure object
        """
        if metrics is None:
            metrics = ['train_score', 'validation_score', 'training_time']
            
        # Extract data for plotting
        models = []
        metric_data = {metric: [] for metric in metrics}
        
        for model_name, result in results.items():
            if hasattr(result, 'train_score'):  # Skip ensemble if needed
                models.append(model_name)
                for metric in metrics:
                    value = getattr(result, metric, 0)
                    metric_data[metric].append(value)
        
        # Create subplots
        n_metrics = len(metrics)
        fig, axes = plt.subplots(2, 2, figsize=(15, 12), dpi=self.dpi)
        fig.suptitle('ðŸ¤– Model Performance Comparison', fontsize=16, fontweight='bold')
        
        # 1. Performance Bar Chart
        ax1 = axes[0, 0]
        x = np.arange(len(models))
        width = 0.35
        
        train_scores = metric_data.get('train_score', [0] * len(models))
        val_scores = metric_data.get('validation_score', [0] * len(models))
        
        bars1 = ax1.bar(x - width/2, train_scores, width, label='Train Score', 
                       color=self.colors[0], alpha=0.8)
        bars2 = ax1.bar(x + width/2, val_scores, width, label='Validation Score',
                       color=self.colors[1], alpha=0.8)
        
        ax1.set_xlabel('Models')
        ax1.set_ylabel('Accuracy Score')
        ax1.set_title('ðŸ“Š Train vs Validation Performance')
        ax1.set_xticks(x)
        ax1.set_xticklabels(models, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar in bars1:
            height = bar.get_height()
            ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                        xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')
        for bar in bars2:
            height = bar.get_height()
            ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),
                        xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')
        
        # 2. Training Time Comparison
        ax2 = axes[0, 1]
        training_times = metric_data.get('training_time', [0] * len(models))
        bars = ax2.bar(models, training_times, color=self.colors[2], alpha=0.8)
        ax2.set_ylabel('Training Time (seconds)')
        ax2.set_title('â±ï¸ Training Time Comparison')
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3)
        
        # Add value labels
        for bar in bars:
            height = bar.get_height()
            ax2.annotate(f'{height:.1f}s', xy=(bar.get_x() + bar.get_width()/2, height),
                        xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')
        
        # 3. Model Ranking Radar Chart
        ax3 = axes[1, 0]
        self._create_radar_chart(ax3, results, models)
        
        # 4. Performance Distribution
        ax4 = axes[1, 1]
        if val_scores:
            ax4.hist(val_scores, bins=min(len(val_scores), 10), alpha=0.7, 
                    color=self.colors[3], edgecolor='black')
            ax4.axvline(np.mean(val_scores), color='red', linestyle='--', 
                       label=f'Mean: {np.mean(val_scores):.3f}')
            ax4.set_xlabel('Validation Score')
            ax4.set_ylabel('Count')
            ax4.set_title('ðŸ“ˆ Performance Distribution')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
            print(f"ðŸ“ Performance comparison saved to: {save_path}")
            
        return fig
    
    def plot_ensemble_weights(self,
                            ensemble_weights: Dict[str, float],
                            save_path: Optional[str] = None) -> plt.Figure:
        """
        Visualize ensemble model weights.
        
        Args:
            ensemble_weights: Dictionary of model weights
            save_path: Path to save the figure
            
        Returns:
            Matplotlib figure object
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), dpi=self.dpi)
        fig.suptitle('ðŸŽ¯ Ensemble Model Weights', fontsize=16, fontweight='bold')
        
        models = list(ensemble_weights.keys())
        weights = list(ensemble_weights.values())
        
        # 1. Bar chart
        bars = ax1.bar(models, weights, color=self.colors[:len(models)], alpha=0.8)
        ax1.set_ylabel('Weight')
        ax1.set_title('ðŸ“Š Model Contribution Weights')
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # Add percentage labels
        for bar, weight in zip(bars, weights):
            height = bar.get_height()
            ax1.annotate(f'{weight:.1%}', xy=(bar.get_x() + bar.get_width()/2, height),
                        xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')
        
        # 2. Pie chart
        ax2.pie(weights, labels=models, autopct='%1.1f%%', startangle=90,
               colors=self.colors[:len(models)])
        ax2.set_title('ðŸ¥§ Weight Distribution')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
            print(f"ðŸ“ Ensemble weights saved to: {save_path}")
            
        return fig
    
    def plot_feature_importance(self,
                              results: Dict[str, Any],
                              top_k: int = 15,
                              save_path: Optional[str] = None) -> plt.Figure:
        """
        Plot feature importance across models.
        
        Args:
            results: Model results with feature importance
            top_k: Number of top features to show
            save_path: Path to save figure
            
        Returns:
            Matplotlib figure object
        """
        fig, axes = plt.subplots(2, 2, figsize=(16, 12), dpi=self.dpi)
        fig.suptitle('ðŸ” Feature Importance Analysis', fontsize=16, fontweight='bold')
        
        model_names = [name for name, result in results.items() 
                      if hasattr(result, 'feature_importance') and result.feature_importance]
        
        for idx, (model_name, result) in enumerate(results.items()):
            if idx >= 4 or not hasattr(result, 'feature_importance') or not result.feature_importance:
                continue
                
            ax = axes[idx // 2, idx % 2]
            
            # Get feature importance
            importance = result.feature_importance
            if isinstance(importance, dict):
                features = list(importance.keys())[:top_k]
                values = list(importance.values())[:top_k]
            else:
                features = [f'Feature_{i}' for i in range(min(top_k, len(importance)))]
                values = importance[:top_k]
            
            # Sort by importance
            sorted_pairs = sorted(zip(features, values), key=lambda x: x[1], reverse=True)
            features, values = zip(*sorted_pairs) if sorted_pairs else ([], [])
            
            # Create horizontal bar chart
            y_pos = np.arange(len(features))
            bars = ax.barh(y_pos, values, color=self.colors[idx % len(self.colors)], alpha=0.8)
            
            ax.set_yticks(y_pos)
            ax.set_yticklabels(features)
            ax.invert_yaxis()
            ax.set_xlabel('Importance')
            ax.set_title(f'ðŸ“ˆ {model_name.replace("_", " ").title()}')
            ax.grid(True, alpha=0.3)
            
            # Add value labels
            for bar, value in zip(bars, values):
                width = bar.get_width()
                ax.annotate(f'{value:.3f}', xy=(width, bar.get_y() + bar.get_height()/2),
                           xytext=(3, 0), textcoords="offset points", va='center')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
            print(f"ðŸ“ Feature importance saved to: {save_path}")
            
        return fig
    
    def plot_prediction_confidence(self,
                                 results: Dict[str, Any],
                                 test_data: Optional[np.ndarray] = None,
                                 save_path: Optional[str] = None) -> plt.Figure:
        """
        Plot prediction confidence distributions.
        
        Args:
            results: Model results
            test_data: Test data for predictions
            save_path: Path to save figure
            
        Returns:
            Matplotlib figure object
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10), dpi=self.dpi)
        fig.suptitle('ðŸŽ¯ Prediction Confidence Analysis', fontsize=16, fontweight='bold')
        
        # This would need actual predictions to work properly
        # For now, create placeholder visualization
        for idx, ax in enumerate(axes.flat):
            # Simulate confidence distributions
            confidence_scores = np.random.beta(2, 2, 1000)  # Simulated for demo
            
            ax.hist(confidence_scores, bins=30, alpha=0.7, 
                   color=self.colors[idx % len(self.colors)], edgecolor='black')
            ax.set_xlabel('Confidence Score')
            ax.set_ylabel('Frequency')
            ax.set_title(f'Model {idx + 1} Confidence')
            ax.grid(True, alpha=0.3)
            
            # Add statistics
            mean_conf = np.mean(confidence_scores)
            ax.axvline(mean_conf, color='red', linestyle='--', 
                      label=f'Mean: {mean_conf:.3f}')
            ax.legend()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
            print(f"ðŸ“ Confidence analysis saved to: {save_path}")
            
        return fig
    
    def _create_radar_chart(self, ax, results: Dict[str, Any], models: List[str]):
        """Create radar chart for model comparison."""
        # Metrics for radar chart
        metrics = ['Accuracy', 'Speed', 'Stability']
        
        # Normalize metrics to 0-1 scale
        data = []
        for model in models:
            result = results[model]
            accuracy = getattr(result, 'validation_score', 0)
            speed = 1.0 / (getattr(result, 'training_time', 1) + 1)  # Inverse time
            stability = 1.0 - abs(getattr(result, 'train_score', 0) - accuracy)  # Less overfitting = more stable
            data.append([accuracy, speed, stability])
        
        # Number of metrics
        N = len(metrics)
        
        # Compute angle for each metric
        theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)
        theta = np.concatenate((theta, [theta[0]]))  # Complete the circle
        
        ax.set_theta_offset(np.pi / 2)
        ax.set_theta_direction(-1)
        ax.set_thetagrids(np.degrees(theta[:-1]), metrics)
        
        for i, (model, model_data) in enumerate(zip(models, data)):
            # Close the plot
            model_data_closed = model_data + [model_data[0]]
            ax.plot(theta, model_data_closed, 'o-', linewidth=2, 
                   label=model, color=self.colors[i % len(self.colors)])
            ax.fill(theta, model_data_closed, alpha=0.25, 
                   color=self.colors[i % len(self.colors)])
        
        ax.set_ylim(0, 1)
        ax.set_title('âš¡ Model Performance Radar')
        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
        ax.grid(True)
    
    def create_performance_dashboard(self,
                                   results: Dict[str, Any],
                                   ensemble_weights: Optional[Dict[str, float]] = None,
                                   save_dir: Optional[str] = None) -> Dict[str, plt.Figure]:
        """
        Create complete performance dashboard.
        
        Args:
            results: Model training results
            ensemble_weights: Ensemble model weights
            save_dir: Directory to save all figures
            
        Returns:
            Dictionary of figure objects
        """
        figures = {}
        
        if save_dir:
            Path(save_dir).mkdir(parents=True, exist_ok=True)
        
        # 1. Model comparison
        print("ðŸ”„ Creating model comparison chart...")
        save_path = f"{save_dir}/model_comparison.png" if save_dir else None
        figures['comparison'] = self.plot_model_comparison(results, save_path=save_path)
        
        # 2. Feature importance
        print("ðŸ”„ Creating feature importance analysis...")
        save_path = f"{save_dir}/feature_importance.png" if save_dir else None
        figures['features'] = self.plot_feature_importance(results, save_path=save_path)
        
        # 3. Ensemble weights (if available)
        if ensemble_weights:
            print("ðŸ”„ Creating ensemble weights visualization...")
            save_path = f"{save_dir}/ensemble_weights.png" if save_dir else None
            figures['ensemble'] = self.plot_ensemble_weights(ensemble_weights, save_path=save_path)
        
        # 4. Prediction confidence
        print("ðŸ”„ Creating confidence analysis...")
        save_path = f"{save_dir}/confidence_analysis.png" if save_dir else None
        figures['confidence'] = self.plot_prediction_confidence(results, save_path=save_path)
        
        print(f"âœ… Performance dashboard created with {len(figures)} visualizations")
        
        return figures
