{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdfbbb8",
   "metadata": {},
   "source": [
    "# Crypto HFT Trading Strategy - Model Development\n",
    "\n",
    "This notebook implements machine learning models for high-frequency trading prediction, following the SGX methodology with ensemble approaches and rolling model selection.\n",
    "\n",
    "## Objectives:\n",
    "1. Train multiple ML models on engineered features\n",
    "2. Implement ensemble methods and model stacking\n",
    "3. Perform hyperparameter optimization\n",
    "4. Evaluate model performance with proper time-series validation\n",
    "5. Implement rolling model selection for live trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65396ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    AdaBoostClassifier, AdaBoostRegressor\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "# XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"XGBoost not available\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"LightGBM not available\")\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "# Model evaluation and tuning\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, TimeSeriesSplit,\n",
    "    cross_val_score, validation_curve\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# Import project modules\n",
    "from models.ml_models import MLModelManager\n",
    "from models.model_selection import RollingModelSelection\n",
    "from utils.visualization import OrderBookVisualizer\n",
    "from utils.metrics import PerformanceMetrics\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54a858",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c65164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed features and targets\n",
    "print(\"Loading processed data...\")\n",
    "\n",
    "try:\n",
    "    X_train = pd.read_parquet('../data/processed/X_train_scaled.parquet')\n",
    "    X_val = pd.read_parquet('../data/processed/X_val_scaled.parquet')\n",
    "    X_test = pd.read_parquet('../data/processed/X_test_scaled.parquet')\n",
    "    \n",
    "    y_train = pd.read_parquet('../data/processed/y_train.parquet')\n",
    "    y_val = pd.read_parquet('../data/processed/y_val.parquet')\n",
    "    y_test = pd.read_parquet('../data/processed/y_test.parquet')\n",
    "    \n",
    "    # Load metadata\n",
    "    import json\n",
    "    with open('../data/processed/feature_metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Data loaded successfully\")\n",
    "    print(f\"  Training: {X_train.shape}\")\n",
    "    print(f\"  Validation: {X_val.shape}\")\n",
    "    print(f\"  Test: {X_test.shape}\")\n",
    "    print(f\"  Target variables: {y_train.shape[1]}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please run 02_feature_engineering.ipynb first.\")\n",
    "    raise\n",
    "\n",
    "# Display target variables\n",
    "print(f\"\\nAvailable target variables:\")\n",
    "for i, col in enumerate(y_train.columns):\n",
    "    print(f\"  {i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efef33",
   "metadata": {},
   "source": [
    "## 2. Model Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model manager\n",
    "model_manager = MLModelManager()\n",
    "\n",
    "# Define model configurations inspired by SGX methodology\n",
    "classification_models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "    },\n",
    "    'ExtraTrees': {\n",
    "        'model': ExtraTreesClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True, random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "    },\n",
    "    'MLP': {\n",
    "        'model': MLPClassifier(random_state=42, max_iter=500),\n",
    "        'params': {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'learning_rate': ['constant', 'adaptive']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    classification_models['XGBoost'] = {\n",
    "        'model': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Add LightGBM if available\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    classification_models['LightGBM'] = {\n",
    "        'model': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"Configured {len(classification_models)} classification models:\")\n",
    "for name in classification_models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a833d59",
   "metadata": {},
   "source": [
    "## 3. Model Training and Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, param_grid, X_train, y_train, X_val, y_val, \n",
    "                           model_name, target_name, cv_folds=3):\n",
    "    \"\"\"\n",
    "    Train and evaluate a single model with hyperparameter tuning\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining {model_name} for {target_name}...\")\n",
    "    \n",
    "    # Time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
    "    \n",
    "    # Randomized search for efficiency (use subset of parameter combinations)\n",
    "    search = RandomizedSearchCV(\n",
    "        model, param_grid, cv=tscv, scoring='accuracy',\n",
    "        n_iter=20, random_state=42, n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_model = search.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = best_model.predict(X_train)\n",
    "    val_pred = best_model.predict(X_val)\n",
    "    \n",
    "    # Probabilities for classification\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        train_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "        val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        train_proba = train_pred\n",
    "        val_proba = val_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'target_name': target_name,\n",
    "        'best_params': search.best_params_,\n",
    "        'best_cv_score': search.best_score_,\n",
    "        'train_accuracy': accuracy_score(y_train, train_pred),\n",
    "        'val_accuracy': accuracy_score(y_val, val_pred),\n",
    "        'train_f1': f1_score(y_train, train_pred, average='weighted'),\n",
    "        'val_f1': f1_score(y_val, val_pred, average='weighted'),\n",
    "        'model': best_model,\n",
    "        'train_pred': train_pred,\n",
    "        'val_pred': val_pred,\n",
    "        'train_proba': train_proba,\n",
    "        'val_proba': val_proba\n",
    "    }\n",
    "    \n",
    "    # Add AUC if binary classification\n",
    "    if len(np.unique(y_train)) == 2:\n",
    "        results['train_auc'] = roc_auc_score(y_train, train_proba)\n",
    "        results['val_auc'] = roc_auc_score(y_val, val_proba)\n",
    "    \n",
    "    print(f\"  ✓ CV Score: {search.best_score_:.4f}\")\n",
    "    print(f\"  ✓ Val Accuracy: {results['val_accuracy']:.4f}\")\n",
    "    print(f\"  ✓ Val F1: {results['val_f1']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Select target variables to train on\n",
    "priority_targets = [\n",
    "    'ETH_direction_1',\n",
    "    'ETH_direction_5',\n",
    "    'XBT_direction_1',\n",
    "    'XBT_direction_5',\n",
    "    'ETH_outperforms_1',\n",
    "    'ETH_outperforms_5'\n",
    "]\n",
    "\n",
    "print(f\"Training models on {len(priority_targets)} priority targets...\")\n",
    "print(f\"Targets: {priority_targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb12c3",
   "metadata": {},
   "source": [
    "## 4. Model Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for all results\n",
    "all_results = []\n",
    "trained_models = {}\n",
    "\n",
    "# Train models for each target\n",
    "for target in priority_targets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING MODELS FOR TARGET: {target}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get target data (drop NaN values)\n",
    "    y_train_target = y_train[target].dropna()\n",
    "    y_val_target = y_val[target].dropna()\n",
    "    \n",
    "    # Align features with target\n",
    "    X_train_target = X_train.loc[y_train_target.index]\n",
    "    X_val_target = X_val.loc[y_val_target.index]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train_target)}\")\n",
    "    print(f\"Validation samples: {len(X_val_target)}\")\n",
    "    print(f\"Target distribution: {y_train_target.value_counts().to_dict()}\")\n",
    "    \n",
    "    target_results = []\n",
    "    target_models = {}\n",
    "    \n",
    "    # Train each model\n",
    "    for model_name, config in classification_models.items():\n",
    "        try:\n",
    "            result = train_and_evaluate_model(\n",
    "                model=config['model'],\n",
    "                param_grid=config['params'],\n",
    "                X_train=X_train_target,\n",
    "                y_train=y_train_target,\n",
    "                X_val=X_val_target,\n",
    "                y_val=y_val_target,\n",
    "                model_name=model_name,\n",
    "                target_name=target\n",
    "            )\n",
    "            \n",
    "            target_results.append(result)\n",
    "            target_models[model_name] = result['model']\n",
    "            all_results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error training {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    trained_models[target] = target_models\n",
    "    \n",
    "    # Show top performers for this target\n",
    "    if target_results:\n",
    "        target_df = pd.DataFrame([{k: v for k, v in r.items() if k not in ['model', 'train_pred', 'val_pred', 'train_proba', 'val_proba']} \n",
    "                                for r in target_results])\n",
    "        target_df = target_df.sort_values('val_accuracy', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 3 performers for {target}:\")\n",
    "        for i, (_, row) in enumerate(target_df.head(3).iterrows()):\n",
    "            print(f\"  {i+1}. {row['model_name']}: {row['val_accuracy']:.4f} accuracy, {row['val_f1']:.4f} F1\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODEL TRAINING COMPLETED\")\n",
    "print(f\"Total models trained: {len(all_results)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5be543",
   "metadata": {},
   "source": [
    "## 5. Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results dataframe\n",
    "results_df = pd.DataFrame([{k: v for k, v in r.items() \n",
    "                          if k not in ['model', 'train_pred', 'val_pred', 'train_proba', 'val_proba']} \n",
    "                         for r in all_results])\n",
    "\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Overall best performers\n",
    "print(\"\\nTop 10 Models by Validation Accuracy:\")\n",
    "top_models = results_df.nlargest(10, 'val_accuracy')\n",
    "for i, (_, row) in enumerate(top_models.iterrows()):\n",
    "    print(f\"{i+1:2d}. {row['model_name']:15s} ({row['target_name']:20s}): {row['val_accuracy']:.4f}\")\n",
    "\n",
    "# Model comparison by target\n",
    "print(\"\\nAverage Performance by Model Type:\")\n",
    "model_avg = results_df.groupby('model_name')[['val_accuracy', 'val_f1']].mean().sort_values('val_accuracy', ascending=False)\n",
    "print(model_avg.round(4))\n",
    "\n",
    "# Target difficulty analysis\n",
    "print(\"\\nAverage Performance by Target:\")\n",
    "target_avg = results_df.groupby('target_name')[['val_accuracy', 'val_f1']].mean().sort_values('val_accuracy', ascending=False)\n",
    "print(target_avg.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a517703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Model performance heatmap\n",
    "pivot_acc = results_df.pivot(index='model_name', columns='target_name', values='val_accuracy')\n",
    "sns.heatmap(pivot_acc, annot=True, fmt='.3f', cmap='viridis', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Validation Accuracy by Model and Target')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. F1 score heatmap\n",
    "pivot_f1 = results_df.pivot(index='model_name', columns='target_name', values='val_f1')\n",
    "sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='plasma', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Validation F1 Score by Model and Target')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Model ranking distribution\n",
    "model_counts = results_df['model_name'].value_counts()\n",
    "axes[1, 0].bar(model_counts.index, model_counts.values)\n",
    "axes[1, 0].set_title('Number of Targets per Model')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# 4. Accuracy distribution\n",
    "for model in results_df['model_name'].unique():\n",
    "    model_data = results_df[results_df['model_name'] == model]['val_accuracy']\n",
    "    axes[1, 1].hist(model_data, alpha=0.6, label=model, bins=10)\n",
    "\n",
    "axes[1, 1].set_title('Validation Accuracy Distribution by Model')\n",
    "axes[1, 1].set_xlabel('Validation Accuracy')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c514a",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for tree-based models\n",
    "def extract_feature_importance(models_dict, feature_names):\n",
    "    \"\"\"\n",
    "    Extract and aggregate feature importance from tree-based models\n",
    "    \"\"\"\n",
    "    importance_data = []\n",
    "    \n",
    "    for target, models in models_dict.items():\n",
    "        for model_name, model in models.items():\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "                for i, (feature, importance) in enumerate(zip(feature_names, importances)):\n",
    "                    importance_data.append({\n",
    "                        'target': target,\n",
    "                        'model': model_name,\n",
    "                        'feature': feature,\n",
    "                        'importance': importance,\n",
    "                        'rank': i\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(importance_data)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance_df = extract_feature_importance(trained_models, X_train.columns)\n",
    "\n",
    "if not feature_importance_df.empty:\n",
    "    # Aggregate importance across models and targets\n",
    "    agg_importance = feature_importance_df.groupby('feature')['importance'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    agg_importance = agg_importance.sort_values('mean', ascending=False)\n",
    "    \n",
    "    print(\"TOP 20 MOST IMPORTANT FEATURES:\")\n",
    "    print(\"=\"*60)\n",
    "    for i, (_, row) in enumerate(agg_importance.head(20).iterrows()):\n",
    "        print(f\"{i+1:2d}. {row['feature']:40s} {row['mean']:.6f} (±{row['std']:.6f})\")\n",
    "    \n",
    "    # Plot top features\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Top 20 features\n",
    "    top_20 = agg_importance.head(20)\n",
    "    axes[0].barh(range(len(top_20)), top_20['mean'])\n",
    "    axes[0].set_yticks(range(len(top_20)))\n",
    "    axes[0].set_yticklabels(top_20['feature'], fontsize=8)\n",
    "    axes[0].set_xlabel('Average Feature Importance')\n",
    "    axes[0].set_title('Top 20 Features by Average Importance')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Feature importance by category\n",
    "    feature_categories = {\n",
    "        'ETH': [f for f in agg_importance['feature'] if f.startswith('ETH_')],\n",
    "        'XBT': [f for f in agg_importance['feature'] if f.startswith('XBT_')],\n",
    "        'Cross-Asset': [f for f in agg_importance['feature'] if any(x in f for x in ['correlation', 'ratio', 'differential', 'lead'])],\n",
    "        'Time': [f for f in agg_importance['feature'] if any(x in f for x in ['hour', 'minute', 'session'])],\n",
    "        'Technical': [f for f in agg_importance['feature'] if any(x in f for x in ['rsi', 'macd', 'bb_', 'momentum'])]\n",
    "    }\n",
    "    \n",
    "    category_importance = {}\n",
    "    for category, features in feature_categories.items():\n",
    "        category_features = agg_importance[agg_importance['feature'].isin(features)]\n",
    "        if not category_features.empty:\n",
    "            category_importance[category] = category_features['mean'].sum()\n",
    "    \n",
    "    if category_importance:\n",
    "        categories = list(category_importance.keys())\n",
    "        importance_values = list(category_importance.values())\n",
    "        \n",
    "        axes[1].pie(importance_values, labels=categories, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1].set_title('Feature Importance by Category')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importance data available (no tree-based models trained successfully)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b675089",
   "metadata": {},
   "source": [
    "## 7. Ensemble Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "def create_ensemble_models(trained_models_dict, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Create ensemble models from the best performing individual models\n",
    "    \"\"\"\n",
    "    ensemble_results = {}\n",
    "    \n",
    "    for target, models in trained_models_dict.items():\n",
    "        if len(models) < 2:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nCreating ensemble for {target}...\")\n",
    "        \n",
    "        # Get aligned data\n",
    "        y_train_target = y_train[target].dropna()\n",
    "        y_val_target = y_val[target].dropna()\n",
    "        X_train_target = X_train.loc[y_train_target.index]\n",
    "        X_val_target = X_val.loc[y_val_target.index]\n",
    "        \n",
    "        # Select top 3 models for ensemble\n",
    "        target_results = [r for r in all_results if r['target_name'] == target]\n",
    "        top_3_models = sorted(target_results, key=lambda x: x['val_accuracy'], reverse=True)[:3]\n",
    "        \n",
    "        # Create voting classifier\n",
    "        estimators = [(result['model_name'], models[result['model_name']]) \n",
    "                     for result in top_3_models if result['model_name'] in models]\n",
    "        \n",
    "        if len(estimators) >= 2:\n",
    "            # Hard voting ensemble\n",
    "            hard_ensemble = VotingClassifier(estimators=estimators, voting='hard')\n",
    "            hard_ensemble.fit(X_train_target, y_train_target)\n",
    "            \n",
    "            # Soft voting ensemble (if all models support predict_proba)\n",
    "            try:\n",
    "                soft_ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "                soft_ensemble.fit(X_train_target, y_train_target)\n",
    "                \n",
    "                # Evaluate both ensembles\n",
    "                hard_pred = hard_ensemble.predict(X_val_target)\n",
    "                soft_pred = soft_ensemble.predict(X_val_target)\n",
    "                \n",
    "                hard_acc = accuracy_score(y_val_target, hard_pred)\n",
    "                soft_acc = accuracy_score(y_val_target, soft_pred)\n",
    "                \n",
    "                ensemble_results[target] = {\n",
    "                    'hard_ensemble': hard_ensemble,\n",
    "                    'soft_ensemble': soft_ensemble,\n",
    "                    'hard_accuracy': hard_acc,\n",
    "                    'soft_accuracy': soft_acc,\n",
    "                    'component_models': [name for name, _ in estimators]\n",
    "                }\n",
    "                \n",
    "                print(f\"  Hard Voting Accuracy: {hard_acc:.4f}\")\n",
    "                print(f\"  Soft Voting Accuracy: {soft_acc:.4f}\")\n",
    "                print(f\"  Component Models: {[name for name, _ in estimators]}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Soft voting failed: {e}\")\n",
    "                # Fallback to hard voting only\n",
    "                hard_pred = hard_ensemble.predict(X_val_target)\n",
    "                hard_acc = accuracy_score(y_val_target, hard_pred)\n",
    "                \n",
    "                ensemble_results[target] = {\n",
    "                    'hard_ensemble': hard_ensemble,\n",
    "                    'hard_accuracy': hard_acc,\n",
    "                    'component_models': [name for name, _ in estimators]\n",
    "                }\n",
    "                \n",
    "                print(f\"  Hard Voting Accuracy: {hard_acc:.4f}\")\n",
    "    \n",
    "    return ensemble_results\n",
    "\n",
    "# Create ensemble models\n",
    "print(\"CREATING ENSEMBLE MODELS\")\n",
    "print(\"=\"*40)\n",
    "ensemble_models = create_ensemble_models(trained_models, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(f\"\\nCreated ensembles for {len(ensemble_models)} targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69807014",
   "metadata": {},
   "source": [
    "## 8. Rolling Model Selection Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aa25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize rolling model selection\n",
    "rolling_selector = RollingModelSelection(\n",
    "    models=list(classification_models.keys()),\n",
    "    window_size=1000,  # 1000 samples per window\n",
    "    retrain_frequency=100  # Retrain every 100 samples\n",
    ")\n",
    "\n",
    "def simulate_rolling_model_selection(target_name, X_data, y_data, test_size=500):\n",
    "    \"\"\"\n",
    "    Simulate rolling model selection for a specific target\n",
    "    \"\"\"\n",
    "    print(f\"\\nSimulating rolling selection for {target_name}...\")\n",
    "    \n",
    "    # Get target data\n",
    "    y_target = y_data[target_name].dropna()\n",
    "    X_target = X_data.loc[y_target.index]\n",
    "    \n",
    "    # Take last test_size samples for simulation\n",
    "    if len(X_target) > test_size:\n",
    "        X_sim = X_target.iloc[-test_size:]\n",
    "        y_sim = y_target.iloc[-test_size:]\n",
    "    else:\n",
    "        X_sim = X_target\n",
    "        y_sim = y_target\n",
    "    \n",
    "    print(f\"  Simulation samples: {len(X_sim)}\")\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    predictions = []\n",
    "    model_selections = []\n",
    "    performance_history = []\n",
    "    \n",
    "    # Rolling simulation\n",
    "    window_size = min(200, len(X_sim) // 3)  # Adaptive window size\n",
    "    \n",
    "    for i in range(window_size, len(X_sim), 10):  # Step by 10 for efficiency\n",
    "        # Training window\n",
    "        X_window = X_sim.iloc[i-window_size:i]\n",
    "        y_window = y_sim.iloc[i-window_size:i]\n",
    "        \n",
    "        # Next sample to predict\n",
    "        if i < len(X_sim):\n",
    "            X_next = X_sim.iloc[[i]]\n",
    "            y_next = y_sim.iloc[i]\n",
    "            \n",
    "            # Train a simple model for this window (for speed)\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            model.fit(X_window, y_window)\n",
    "            \n",
    "            # Make prediction\n",
    "            pred = model.predict(X_next)[0]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Calculate accuracy for this window\n",
    "            window_pred = model.predict(X_window)\n",
    "            window_acc = accuracy_score(y_window, window_pred)\n",
    "            \n",
    "            performance_history.append({\n",
    "                'step': i,\n",
    "                'window_accuracy': window_acc,\n",
    "                'prediction': pred,\n",
    "                'actual': y_next,\n",
    "                'correct': pred == y_next\n",
    "            })\n",
    "    \n",
    "    # Calculate overall performance\n",
    "    if performance_history:\n",
    "        overall_accuracy = np.mean([p['correct'] for p in performance_history])\n",
    "        print(f\"  Rolling prediction accuracy: {overall_accuracy:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'target': target_name,\n",
    "            'accuracy': overall_accuracy,\n",
    "            'predictions': len(predictions),\n",
    "            'performance_history': performance_history\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Simulate rolling selection for key targets\n",
    "rolling_results = []\n",
    "test_targets = ['ETH_direction_1', 'ETH_direction_5', 'XBT_direction_1']\n",
    "\n",
    "print(\"ROLLING MODEL SELECTION SIMULATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for target in test_targets:\n",
    "    result = simulate_rolling_model_selection(target, X_val, y_val)\n",
    "    if result:\n",
    "        rolling_results.append(result)\n",
    "\n",
    "print(f\"\\nCompleted rolling simulation for {len(rolling_results)} targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2bb94",
   "metadata": {},
   "source": [
    "## 9. Model Persistence and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1561404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models and results with error handling\n",
    "import os\n",
    "os.makedirs('../models/trained', exist_ok=True)\n",
    "os.makedirs('../models/ensembles', exist_ok=True)\n",
    "try:\n",
    "    # Save individual models\n",
    "    for target, models in trained_models.items():\n",
    "        for model_name, model in models.items():\n",
    "            filename = f\"../models/trained/{target}_{model_name}.pkl\"\n",
    "            joblib.dump(model, filename)\n",
    "    # Save ensemble models\n",
    "    for target, ensemble_data in ensemble_models.items():\n",
    "        if 'hard_ensemble' in ensemble_data:\n",
    "            joblib.dump(ensemble_data['hard_ensemble'], f\"../models/ensembles/{target}_hard_ensemble.pkl\")\n",
    "        if 'soft_ensemble' in ensemble_data:\n",
    "            joblib.dump(ensemble_data['soft_ensemble'], f\"../models/ensembles/{target}_soft_ensemble.pkl\")\n",
    "    # Save results and metadata\n",
    "    results_df.to_csv('../data/processed/model_training_results.csv', index=False)\n",
    "    results_df.to_parquet('../data/processed/model_training_results.parquet')\n",
    "    with open('../models/model_registry.json', 'w') as f:\n",
    "        json.dump(model_registry, f, indent=2)\n",
    "    with open('../models/ensemble_registry.json', 'w') as f:\n",
    "        json.dump(ensemble_registry, f, indent=2)\n",
    "    with open('../data/processed/training_summary.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    if not feature_importance_df.empty:\n",
    "        feature_importance_df.to_csv('../data/processed/feature_importance.csv', index=False)\n",
    "        agg_importance.to_csv('../data/processed/aggregated_feature_importance.csv', index=False)\n",
    "    print(\"✓ Model persistence completed. All files saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving models or results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fabd9a",
   "metadata": {},
   "source": [
    "## 10. Model Development Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d518ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODEL DEVELOPMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. TRAINING STATISTICS:\")\n",
    "print(f\"   Total models trained: {len(all_results)}\")\n",
    "print(f\"   Model types: {len(classification_models)}\")\n",
    "print(f\"   Target variables: {len(set([r['target_name'] for r in all_results]))}\")\n",
    "print(f\"   Ensemble models created: {len(ensemble_models)}\")\n",
    "\n",
    "print(f\"\\n2. PERFORMANCE HIGHLIGHTS:\")\n",
    "if not results_df.empty:\n",
    "    best_model = results_df.loc[results_df['val_accuracy'].idxmax()]\n",
    "    print(f\"   Best single model: {best_model['model_name']} on {best_model['target_name']}\")\n",
    "    print(f\"   Best accuracy: {best_model['val_accuracy']:.4f}\")\n",
    "    print(f\"   Average accuracy: {results_df['val_accuracy'].mean():.4f}\")\n",
    "    print(f\"   Accuracy std: {results_df['val_accuracy'].std():.4f}\")\n",
    "\n",
    "print(f\"\\n3. MODEL TYPE PERFORMANCE:\")\n",
    "for model_type in model_avg.index:\n",
    "    print(f\"   {model_type:15s}: {model_avg.loc[model_type, 'val_accuracy']:.4f} avg accuracy\")\n",
    "\n",
    "print(f\"\\n4. TARGET DIFFICULTY RANKING:\")\n",
    "for target in target_avg.index:\n",
    "    print(f\"   {target:25s}: {target_avg.loc[target, 'val_accuracy']:.4f} avg accuracy\")\n",
    "\n",
    "if ensemble_models:\n",
    "    print(f\"\\n5. ENSEMBLE PERFORMANCE:\")\n",
    "    for target, ensemble_data in ensemble_models.items():\n",
    "        if 'soft_accuracy' in ensemble_data:\n",
    "            print(f\"   {target:25s}: {ensemble_data['soft_accuracy']:.4f} (soft voting)\")\n",
    "        else:\n",
    "            print(f\"   {target:25s}: {ensemble_data['hard_accuracy']:.4f} (hard voting)\")\n",
    "\n",
    "if rolling_results:\n",
    "    print(f\"\\n6. ROLLING SIMULATION RESULTS:\")\n",
    "    for result in rolling_results:\n",
    "        print(f\"   {result['target']:20s}: {result['accuracy']:.4f} accuracy ({result['predictions']} predictions)\")\n",
    "\n",
    "print(f\"\\n7. FEATURE INSIGHTS:\")\n",
    "if not feature_importance_df.empty:\n",
    "    print(f\"   Total features analyzed: {len(agg_importance)}\")\n",
    "    print(f\"   Top feature: {agg_importance.iloc[0]['feature']}\")\n",
    "    print(f\"   Top importance: {agg_importance.iloc[0]['mean']:.6f}\")\n",
    "    \n",
    "    # Feature category insights\n",
    "    eth_features = len([f for f in agg_importance['feature'] if f.startswith('ETH_')])\n",
    "    xbt_features = len([f for f in agg_importance['feature'] if f.startswith('XBT_')])\n",
    "    cross_features = len([f for f in agg_importance['feature'] if any(x in f for x in ['correlation', 'ratio', 'differential'])])\n",
    "    \n",
    "    print(f\"   ETH-specific features: {eth_features}\")\n",
    "    print(f\"   XBT-specific features: {xbt_features}\")\n",
    "    print(f\"   Cross-asset features: {cross_features}\")\n",
    "\n",
    "print(f\"\\n8. NEXT STEPS:\")\n",
    "print(f\"   ✓ Models ready for strategy backtesting\")\n",
    "print(f\"   ✓ Ensemble models available for improved predictions\")\n",
    "print(f\"   ✓ Rolling selection framework implemented\")\n",
    "print(f\"   ✓ Feature importance analysis completed\")\n",
    "print(f\"   → Ready for strategy implementation and backtesting\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"MODEL DEVELOPMENT COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"All models and results saved to respective directories.\")\n",
    "print(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global seed for reproducibility\n",
    "import random\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "try:\n",
    "    import os\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert required columns exist before model training\n",
    "assert X_train.shape[0] > 0 and X_val.shape[0] > 0 and X_test.shape[0] > 0, \"Feature datasets are empty!\"\n",
    "assert y_train.shape[0] > 0 and y_val.shape[0] > 0 and y_test.shape[0] > 0, \"Target datasets are empty!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models and results with error handling\n",
    "import os\n",
    "os.makedirs('../models/trained', exist_ok=True)\n",
    "os.makedirs('../models/ensembles', exist_ok=True)\n",
    "try:\n",
    "    # Save individual models\n",
    "    for target, models in trained_models.items():\n",
    "        for model_name, model in models.items():\n",
    "            filename = f\"../models/trained/{target}_{model_name}.pkl\"\n",
    "            joblib.dump(model, filename)\n",
    "    # Save ensemble models\n",
    "    for target, ensemble_data in ensemble_models.items():\n",
    "        if 'hard_ensemble' in ensemble_data:\n",
    "            joblib.dump(ensemble_data['hard_ensemble'], f\"../models/ensembles/{target}_hard_ensemble.pkl\")\n",
    "        if 'soft_ensemble' in ensemble_data:\n",
    "            joblib.dump(ensemble_data['soft_ensemble'], f\"../models/ensembles/{target}_soft_ensemble.pkl\")\n",
    "    # Save results and metadata\n",
    "    results_df.to_csv('../data/processed/model_training_results.csv', index=False)\n",
    "    results_df.to_parquet('../data/processed/model_training_results.parquet')\n",
    "    with open('../models/model_registry.json', 'w') as f:\n",
    "        json.dump(model_registry, f, indent=2)\n",
    "    with open('../models/ensemble_registry.json', 'w') as f:\n",
    "        json.dump(ensemble_registry, f, indent=2)\n",
    "    with open('../data/processed/training_summary.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    if not feature_importance_df.empty:\n",
    "        feature_importance_df.to_csv('../data/processed/feature_importance.csv', index=False)\n",
    "        agg_importance.to_csv('../data/processed/aggregated_feature_importance.csv', index=False)\n",
    "    print(\"✓ Model persistence completed. All files saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving models or results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple output validation (unit-test style)\n",
    "assert results_df.shape[0] > 0, \"No model results generated!\"\n",
    "if not feature_importance_df.empty:\n",
    "    assert agg_importance.shape[0] > 0, \"No feature importance aggregated!\"\n",
    "print(\"[TESTS PASSED] Key outputs are valid.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
