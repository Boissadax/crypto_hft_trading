{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c77e867",
   "metadata": {},
   "source": [
    "# Alpha Signal Comparison Notebook\n",
    "\n",
    "Ce notebook a pour objectif de comparer systématiquement différents signaux et méthodes statistiques pour la prédiction court terme sur cryptomonnaies. Il s'appuie sur tous les modules du projet pour garantir robustesse et reproductibilité.\n",
    "\n",
    "## Plan :\n",
    "1. Chargement des features extraites\n",
    "2. Synchronisation des séries temporelles\n",
    "3. Boucle sur signaux et méthodes statistiques\n",
    "4. Calcul des scores (valeur, p-value, lag optimal, robustesse)\n",
    "5. Visualisations (heatmaps, distributions, etc.)\n",
    "6. Tableau de synthèse et documentation des résultats\n",
    "7. Export des meilleurs signaux/paramètres pour l'équipe backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caaedb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies principales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm  # Ajout pour la barre de chargement\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import des modules du projet\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from feature_engineering.order_book_features import OrderBookFeatureExtractor\n",
    "from feature_engineering.time_series_features import TimeSeriesFeatureExtractor\n",
    "from feature_engineering.synchronization import AsynchronousSync, SyncConfig\n",
    "from statistical_analysis.transfer_entropy import TransferEntropyAnalyzer\n",
    "from statistical_analysis.causality_tests import CausalityTester\n",
    "from statistical_analysis.correlation_analysis import CrossCorrelationAnalyzer\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb0e77",
   "metadata": {},
   "source": [
    "## 1. Chargement des données features extraites\n",
    "\n",
    "On charge les features déjà extraites (order book, time series, etc.) depuis le dossier `processed_data/` ou `data_cache/` pour éviter tout retraitement inutile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c667d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers de features détectés : {'ETH': '../data_cache/ETH_4a25fd7dc5f00c1639dfde83bf483c73.parquet', 'BTC': '../data_cache/BTC_e5b6226914df159d942f4575aa84f65d.parquet'}\n",
      "ETH: shape=(53700040, 6), colonnes=['timestamp_us', 'symbol', 'price', 'volume', 'side', 'level'] ...\n",
      "BTC: shape=(17601860, 6), colonnes=['timestamp_us', 'symbol', 'price', 'volume', 'side', 'level'] ...\n",
      "ETH: shape=(53700040, 6), colonnes=['timestamp_us', 'symbol', 'price', 'volume', 'side', 'level'] ...\n",
      "BTC: shape=(17601860, 6), colonnes=['timestamp_us', 'symbol', 'price', 'volume', 'side', 'level'] ...\n"
     ]
    }
   ],
   "source": [
    "# Détection automatique des fichiers de features disponibles\n",
    "feature_files = {}\n",
    "data_cache_path = '../data_cache/'\n",
    "processed_data_path = '../processed_data/'\n",
    "\n",
    "# Recherche dans data_cache (parquet)\n",
    "for fname in os.listdir(data_cache_path):\n",
    "    if fname.endswith('.parquet'):\n",
    "        symbol = fname.split('_')[0]\n",
    "        feature_files[symbol] = os.path.join(data_cache_path, fname)\n",
    "\n",
    "# Recherche dans processed_data (csv ou parquet)\n",
    "for fname in os.listdir(processed_data_path):\n",
    "    if fname.endswith('.parquet') or fname.endswith('.csv'):\n",
    "        symbol = fname.split('_')[0]\n",
    "        feature_files[symbol] = os.path.join(processed_data_path, fname)\n",
    "\n",
    "print(f\"Fichiers de features détectés : {feature_files}\")\n",
    "\n",
    "# Chargement des DataFrames\n",
    "crypto_data = {}\n",
    "for symbol, path in feature_files.items():\n",
    "    if path.endswith('.parquet'):\n",
    "        crypto_data[symbol] = pd.read_parquet(path)\n",
    "    elif path.endswith('.csv'):\n",
    "        crypto_data[symbol] = pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "    else:\n",
    "        print(f\"Format non supporté pour {path}\")\n",
    "\n",
    "for symbol, df in crypto_data.items():\n",
    "    print(f\"{symbol}: shape={df.shape}, colonnes={list(df.columns)[:8]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a559956",
   "metadata": {},
   "source": [
    "## 2. Synchronisation asynchrone des séries temporelles\n",
    "\n",
    "On synchronise les séries de tous les symboles sans arrondir les timestamps, en utilisant la classe AsynchronousSync du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8327714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de la synchronisation\n",
    "symbols = list(crypto_data.keys())\n",
    "sync_config = SyncConfig(\n",
    "    tolerance_us=1_000_000,  # 1 seconde\n",
    "    interpolation_method='linear',\n",
    "    resampling_frequency_us=1_000_000,\n",
    "    enable_cross_symbol_features=True\n",
    ")\n",
    "synchronizer = AsynchronousSync(config=sync_config, symbols=symbols)\n",
    "\n",
    "# Synchronisation vectorisée des séries temporelles (sans boucle ligne à ligne)\n",
    "dfs = []\n",
    "for symbol, df in crypto_data.items():\n",
    "    idx = df.index\n",
    "    # Conversion index en microsecondes\n",
    "    if hasattr(idx, \"to_pydatetime\"):\n",
    "        ts_us = (idx.to_series().apply(lambda x: int(x.timestamp() * 1e6)))\n",
    "    else:\n",
    "        ts_us = pd.Series(idx.values.astype(np.int64), index=idx)\n",
    "        if ts_us.max() < 1e12:\n",
    "            ts_us = ts_us * 1_000_000\n",
    "    df = df.copy()\n",
    "    df['timestamp_us'] = ts_us.values\n",
    "    df = df.reset_index(drop=True)\n",
    "    # On préfixe les colonnes pour chaque symbole\n",
    "    df = df.rename(columns={col: f\"{symbol}_{col}\" for col in df.columns if col != 'timestamp_us'})\n",
    "    dfs.append(df[['timestamp_us'] + [c for c in df.columns if c != 'timestamp_us']])\n",
    "\n",
    "# Fusion outer join sur timestamp_us\n",
    "from functools import reduce\n",
    "sync_df = reduce(lambda left, right: pd.merge(left, right, on='timestamp_us', how='outer'), dfs)\n",
    "sync_df = sync_df.sort_values('timestamp_us').reset_index(drop=True)\n",
    "sync_df['timestamp'] = pd.to_datetime(sync_df['timestamp_us'] // 1_000_000, unit='s')\n",
    "sync_df = sync_df.set_index('timestamp')\n",
    "print(f\"DataFrame synchronisé shape: {sync_df.shape}\")\n",
    "\n",
    "# Si besoin, on peut passer sync_df à la classe AsynchronousSync pour interpolation/traitement supplémentaire\n",
    "# synchronizer = AsynchronousSync(config=sync_config, symbols=symbols)\n",
    "# sync_points = synchronizer.process_dataframe(sync_df)  # À adapter selon l'API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161efb9",
   "metadata": {},
   "source": [
    "## 3. Boucle sur signaux et méthodes statistiques\n",
    "\n",
    "Pour chaque signal pertinent (mid_price, spread, imbalance, etc.) et chaque méthode (Transfer Entropy, Granger, corrélation croisée), on calcule la force du signal pour chaque paire de symboles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a31492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des signaux et méthodes à tester\n",
    "SIGNALS = ['mid_price', 'spread', 'imbalance', 'bid_volume_l1', 'ask_volume_l1']\n",
    "METHODS = ['transfer_entropy', 'granger', 'correlation']\n",
    "\n",
    "# Initialisation des analyseurs\n",
    "te_analyzer = TransferEntropyAnalyzer(method='ksg', max_workers=4)\n",
    "granger_tester = CausalityTester()\n",
    "corr_analyzer = CrossCorrelationAnalyzer()\n",
    "\n",
    "results = []\n",
    "\n",
    "for signal in SIGNALS:\n",
    "    # Vérifie que le signal existe pour tous les symboles\n",
    "    available = [f\"{sym}_{signal}\" in sync_df.columns for sym in symbols]\n",
    "    if not all(available):\n",
    "        print(f\"Signal {signal} absent pour certains symboles, ignoré.\")\n",
    "        continue\n",
    "    for i, source in enumerate(symbols):\n",
    "        for j, target in enumerate(symbols):\n",
    "            if i == j:\n",
    "                continue\n",
    "            source_col = f\"{source}_{signal}\"\n",
    "            target_col = f\"{target}_{signal}\"\n",
    "            x = sync_df[source_col].values\n",
    "            y = sync_df[target_col].values\n",
    "            # Nettoyage NaN\n",
    "            mask = np.isfinite(x) & np.isfinite(y)\n",
    "            x = x[mask]\n",
    "            y = y[mask]\n",
    "            if len(x) < 200:\n",
    "                continue\n",
    "            # Méthode Transfer Entropy\n",
    "            try:\n",
    "                te_res = te_analyzer.analyze_transfer_entropy(x, y, source, target, lags_us=[1_000_000, 2_000_000, 5_000_000])\n",
    "                best_te = max(te_res, key=lambda r: r.transfer_entropy)\n",
    "                results.append({\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'signal': signal,\n",
    "                    'method': 'transfer_entropy',\n",
    "                    'score': best_te.transfer_entropy,\n",
    "                    'p_value': best_te.p_value,\n",
    "                    'lag_us': best_te.lag_us,\n",
    "                    'significant': best_te.p_value < 0.05\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"TE erreur {source}->{target} {signal}: {e}\")\n",
    "            # Méthode Granger\n",
    "            try:\n",
    "                granger = granger_tester.granger_causality_test(x, y, max_lags=5)\n",
    "                pval = granger.get('X->Y', {}).get('p_value', 1.0)\n",
    "                results.append({\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'signal': signal,\n",
    "                    'method': 'granger',\n",
    "                    'score': -np.log10(pval+1e-10),\n",
    "                    'p_value': pval,\n",
    "                    'lag_us': None,\n",
    "                    'significant': pval < 0.05\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Granger erreur {source}->{target} {signal}: {e}\")\n",
    "            # Méthode Corrélation croisée\n",
    "            try:\n",
    "                corr = corr_analyzer.cross_correlation(x, y, max_lag=10)\n",
    "                max_corr = np.max(np.abs(corr.correlation))\n",
    "                lag = corr.lags[np.argmax(np.abs(corr.correlation))]\n",
    "                results.append({\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'signal': signal,\n",
    "                    'method': 'correlation',\n",
    "                    'score': max_corr,\n",
    "                    'p_value': None,\n",
    "                    'lag_us': lag*1_000_000,\n",
    "                    'significant': max_corr > 0.2\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Corr erreur {source}->{target} {signal}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"Nombre total de tests réalisés : {len(results_df)}\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446eac1",
   "metadata": {},
   "source": [
    "## 4. Visualisations et synthèse des résultats\n",
    "\n",
    "On visualise les scores des signaux/méthodes et on synthétise les résultats pour identifier les signaux robustes et ceux qui ne fonctionnent pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap des scores par signal et méthode\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "for method in METHODS:\n",
    "    pivot = results_df[results_df['method'] == method].pivot_table(\n",
    "        index=['source', 'target'], columns='signal', values='score', aggfunc='max')\n",
    "    if pivot.empty:\n",
    "        continue\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='viridis', cbar_kws={'label': method})\n",
    "    plt.title(f'Heatmap {method} (score max par signal)')\n",
    "    plt.ylabel('Source → Target')\n",
    "    plt.xlabel('Signal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Distribution des scores et des p-values\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "results_df[results_df['method']=='transfer_entropy']['score'].hist(bins=30, alpha=0.7)\n",
    "plt.title('Distribution Transfer Entropy')\n",
    "plt.xlabel('TE')\n",
    "plt.subplot(1, 2, 2)\n",
    "results_df[results_df['method']=='granger']['p_value'].hist(bins=30, alpha=0.7)\n",
    "plt.title('Distribution p-value Granger')\n",
    "plt.xlabel('p-value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau de synthèse des meilleurs signaux\n",
    "best_signals = results_df[results_df['significant']].groupby(['signal', 'method']).agg(\n",
    "    n=('score', 'count'),\n",
    "    mean_score=('score', 'mean'),\n",
    "    min_pval=('p_value', 'min')\n",
    ").sort_values('mean_score', ascending=False)\n",
    "print(\"Tableau de synthèse des signaux/méthodes les plus robustes :\")\n",
    "display(best_signals.head(10))\n",
    "\n",
    "# Export des meilleurs signaux pour l'équipe backtest\n",
    "export_cols = ['source', 'target', 'signal', 'method', 'score', 'p_value', 'lag_us', 'significant']\n",
    "best_signals_df = results_df[results_df['significant']][export_cols]\n",
    "best_signals_df.to_csv('../results/best_alpha_signals.csv', index=False)\n",
    "print(f\"Exporté {len(best_signals_df)} signaux robustes dans ../results/best_alpha_signals.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4ae6e",
   "metadata": {},
   "source": [
    "## 1. Extraction des features avancées à partir des carnets de commandes\n",
    "\n",
    "Avant la synchronisation, nous extrayons des features avancées (mid_price, spread, imbalance, bid/ask_volume_l1, etc.) pour chaque symbole à partir des données de carnet de commandes de niveau 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af2ac0f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m     order_book_data\u001b[38;5;241m.\u001b[39mappend((symbol, timestamp_us, snapshots))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Extraction batchée des features\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m features_df \u001b[38;5;241m=\u001b[39m ob_extractor\u001b[38;5;241m.\u001b[39mextract_batch_features(order_book_data)\n\u001b[1;32m     44\u001b[0m features_df \u001b[38;5;241m=\u001b[39m features_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp_us\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m crypto_data[symbol] \u001b[38;5;241m=\u001b[39m features_df\n",
      "File \u001b[0;32m~/Documents/GitHub/CRYPTO/hft_engine_v3/notebooks/../feature_engineering/order_book_features.py:350\u001b[0m, in \u001b[0;36mOrderBookFeatureExtractor.extract_batch_features\u001b[0;34m(self, order_book_data, chunk_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m chunk \u001b[38;5;241m=\u001b[39m order_book_data[i:i \u001b[38;5;241m+\u001b[39m chunk_size]\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m symbol, timestamp_us, snapshots \u001b[38;5;129;01min\u001b[39;00m chunk:\n\u001b[0;32m--> 350\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(snapshots, symbol, timestamp_us)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features:\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;66;03m# Convert to dictionary for DataFrame creation\u001b[39;00m\n\u001b[1;32m    353\u001b[0m         feature_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    354\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp_us\u001b[39m\u001b[38;5;124m'\u001b[39m: features\u001b[38;5;241m.\u001b[39mtimestamp_us,\n\u001b[1;32m    355\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m: features\u001b[38;5;241m.\u001b[39msymbol,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask_slope\u001b[39m\u001b[38;5;124m'\u001b[39m: features\u001b[38;5;241m.\u001b[39mask_slope\n\u001b[1;32m    374\u001b[0m         }\n",
      "File \u001b[0;32m~/Documents/GitHub/CRYPTO/hft_engine_v3/notebooks/../feature_engineering/order_book_features.py:121\u001b[0m, in \u001b[0;36mOrderBookFeatureExtractor.extract_features\u001b[0;34m(self, snapshots, symbol, timestamp_us)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_cache[cache_key]\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Organize snapshots by side and level\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m order_book \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_organize_order_book(snapshots)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_valid_order_book(order_book):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextraction_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid_books\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/CRYPTO/hft_engine_v3/notebooks/../feature_engineering/order_book_features.py:153\u001b[0m, in \u001b[0;36mOrderBookFeatureExtractor._organize_order_book\u001b[0;34m(self, snapshots)\u001b[0m\n\u001b[1;32m    150\u001b[0m     side \u001b[38;5;241m=\u001b[39m snapshot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mside\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    151\u001b[0m     level \u001b[38;5;241m=\u001b[39m snapshot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 153\u001b[0m     order_book[side][level] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m'\u001b[39m: snapshot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    156\u001b[0m     }\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m order_book\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extraction vectorisée des features avancées (mid_price, spread, imbalance, bid/ask_volume_l1) pour chaque symbole\n",
    "from feature_engineering.order_book_features import OrderBookFeatureExtractor\n",
    "\n",
    "ob_extractor = OrderBookFeatureExtractor()\n",
    "\n",
    "for symbol, df in crypto_data.items():\n",
    "    # On suppose que chaque ligne du DataFrame correspond à un snapshot de carnet (niveau 1)\n",
    "    # On regroupe les lignes par timestamp_us (ou index) pour former des snapshots\n",
    "    # Ici, on suppose que chaque ligne = un niveau, donc on regroupe par timestamp\n",
    "    if 'timestamp_us' in df.columns:\n",
    "        ts_col = 'timestamp_us'\n",
    "    elif 'timestamp' in df.columns:\n",
    "        ts_col = 'timestamp'\n",
    "    else:\n",
    "        # Si index est datetime ou float/int timestamp\n",
    "        if hasattr(df.index, 'to_pydatetime') or np.issubdtype(df.index.dtype, np.datetime64):\n",
    "            ts_col = None\n",
    "            df = df.reset_index().rename(columns={'index': 'timestamp'})\n",
    "        else:\n",
    "            ts_col = None\n",
    "            df = df.reset_index().rename(columns={'index': 'timestamp'})\n",
    "    \n",
    "    # On regroupe par timestamp pour reconstituer les snapshots\n",
    "    order_book_data = []\n",
    "    for ts, group in df.groupby(ts_col if ts_col else 'timestamp'):\n",
    "        # On convertit le timestamp en microsecondes\n",
    "        if isinstance(ts, pd.Timestamp):\n",
    "            timestamp_us = int(ts.timestamp() * 1e6)\n",
    "        elif isinstance(ts, (float, np.floating)) and ts > 1e12:\n",
    "            timestamp_us = int(ts)\n",
    "        elif isinstance(ts, (float, np.floating)):\n",
    "            timestamp_us = int(ts * 1e6)\n",
    "        elif isinstance(ts, (int, np.integer)) and ts > 1e12:\n",
    "            timestamp_us = ts\n",
    "        elif isinstance(ts, (int, np.integer)):\n",
    "            timestamp_us = ts * 1_000_000\n",
    "        else:\n",
    "            continue\n",
    "        # On convertit le group en liste de dicts (snapshots)\n",
    "        snapshots = group.to_dict('records')\n",
    "        order_book_data.append((symbol, timestamp_us, snapshots))\n",
    "    # Extraction batchée des features\n",
    "    features_df = ob_extractor.extract_batch_features(order_book_data)\n",
    "    features_df = features_df.set_index('timestamp_us')\n",
    "    crypto_data[symbol] = features_df\n",
    "print(\"Features avancées extraites et injectées dans crypto_data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de la synchronisation\n",
    "symbols = list(crypto_data.keys())\n",
    "sync_config = SyncConfig(\n",
    "    tolerance_us=1_000_000,  # 1 seconde\n",
    "    interpolation_method='linear',\n",
    "    resampling_frequency_us=1_000_000,\n",
    "    enable_cross_symbol_features=True\n",
    ")\n",
    "synchronizer = AsynchronousSync(config=sync_config, symbols=symbols)\n",
    "\n",
    "# Synchronisation vectorisée des séries temporelles (sans boucle ligne à ligne)\n",
    "dfs = []\n",
    "for symbol, df in crypto_data.items():\n",
    "    idx = df.index\n",
    "    # Conversion index en microsecondes\n",
    "    if hasattr(idx, \"to_pydatetime\"):\n",
    "        ts_us = (idx.to_series().apply(lambda x: int(x.timestamp() * 1e6)))\n",
    "    else:\n",
    "        ts_us = pd.Series(idx.values.astype(np.int64), index=idx)\n",
    "        if ts_us.max() < 1e12:\n",
    "            ts_us = ts_us * 1_000_000\n",
    "    df = df.copy()\n",
    "    df['timestamp_us'] = ts_us.values\n",
    "    df = df.reset_index(drop=True)\n",
    "    # On préfixe les colonnes pour chaque symbole\n",
    "    df = df.rename(columns={col: f\"{symbol}_{col}\" for col in df.columns if col != 'timestamp_us'})\n",
    "    dfs.append(df[['timestamp_us'] + [c for c in df.columns if c != 'timestamp_us']])\n",
    "\n",
    "# Fusion outer join sur timestamp_us\n",
    "from functools import reduce\n",
    "sync_df = reduce(lambda left, right: pd.merge(left, right, on='timestamp_us', how='outer'), dfs)\n",
    "sync_df = sync_df.sort_values('timestamp_us').reset_index(drop=True)\n",
    "sync_df['timestamp'] = pd.to_datetime(sync_df['timestamp_us'] // 1_000_000, unit='s')\n",
    "sync_df = sync_df.set_index('timestamp')\n",
    "print(f\"DataFrame synchronisé shape: {sync_df.shape}\")\n",
    "\n",
    "# Si besoin, on peut passer sync_df à la classe AsynchronousSync pour interpolation/traitement supplémentaire\n",
    "# synchronizer = AsynchronousSync(config=sync_config, symbols=symbols)\n",
    "# sync_points = synchronizer.process_dataframe(sync_df)  # À adapter selon l'API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des signaux et méthodes à tester\n",
    "SIGNALS = ['mid_price', 'spread', 'imbalance', 'bid_volume_l1', 'ask_volume_l1']\n",
    "METHODS = ['transfer_entropy', 'granger', 'correlation']\n",
    "\n",
    "# Initialisation des analyseurs\n",
    "te_analyzer = TransferEntropyAnalyzer(method='ksg', max_workers=4)\n",
    "granger_tester = CausalityTester()\n",
    "corr_analyzer = CrossCorrelationAnalyzer()\n",
    "\n",
    "results = []\n",
    "\n",
    "for signal in SIGNALS:\n",
    "    # Vérifie que le signal existe pour tous les symboles\n",
    "    available = [f\"{sym}_{signal}\" in sync_df.columns for sym in symbols]\n",
    "    if not all(available):\n",
    "        print(f\"Signal {signal} absent pour certains symboles, ignoré.\")\n",
    "        continue\n",
    "    for i, source in enumerate(symbols):\n",
    "        for j, target in enumerate(symbols):\n",
    "            if i == j:\n",
    "                continue\n",
    "            source_col = f\"{source}_{signal}\"\n",
    "            target_col = f\"{target}_{signal}\"\n",
    "            x = sync_df[source_col].values\n",
    "            y = sync_df[target_col].values\n",
    "            # Nettoyage NaN\n",
    "            mask = np.isfinite(x) & np.isfinite(y)\n",
    "            x = x[mask]\n",
    "            y = y[mask]\n",
    "            if len(x) < 200:\n",
    "                continue\n",
    "            # Méthode Transfer Entropy\n",
    "            try:\n",
    "                te_res = te_analyzer.analyze_transfer_entropy(x, y, source, target, lags_us=[1_000_000, 2_000_000, 5_000_000])\n",
    "                best_te = max(te_res, key=lambda r: r.transfer_entropy)\n",
    "                results.append({\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'signal': signal,\n",
    "                    'method': 'transfer_entropy',\n",
    "                    'score': best_te.transfer_entropy,\n",
    "                    'p_value': best_te.p_value,\n",
    "                    'lag_us': best_te.lag_us,\n",
    "                    'significant': best_te.p_value < 0.05\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"TE erreur {source}->{target} {signal}: {e}\")\n",
    "            # Méthode Granger\n",
    "            try:\n",
    "                granger = granger_tester.granger_causality_test(x, y, max_lags=5)\n",
    "                pval = granger.get('X->Y', {}).get('p_value', 1.0)\n",
    "                results.append({\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'signal': signal,\n",
    "                    'method': 'granger',\n",
    "                    'score': -np.log10(pval+1e-10),\n",
    "                    'p_value': pval,\n",
    "                    'lag_us': None,\n",
    "                    'significant': pval < 0.05\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Granger erreur {source}->{target} {signal}: {e}\")\n",
    "            # Méthode Corrélation croisée\n",
    "            try:\n",
    "                corr = corr_analyzer.cross_correlation(x, y, max_lag=10)\n",
    "                max_corr = np.max(np.abs(corr.correlation))\n",
    "                lag = corr.lags[np.argmax(np.abs(corr.correlation))]\n",
    "                results.append({\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'signal': signal,\n",
    "                    'method': 'correlation',\n",
    "                    'score': max_corr,\n",
    "                    'p_value': None,\n",
    "                    'lag_us': lag*1_000_000,\n",
    "                    'significant': max_corr > 0.2\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Corr erreur {source}->{target} {signal}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"Nombre total de tests réalisés : {len(results_df)}\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e889a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap des scores par signal et méthode\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "for method in METHODS:\n",
    "    pivot = results_df[results_df['method'] == method].pivot_table(\n",
    "        index=['source', 'target'], columns='signal', values='score', aggfunc='max')\n",
    "    if pivot.empty:\n",
    "        continue\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='viridis', cbar_kws={'label': method})\n",
    "    plt.title(f'Heatmap {method} (score max par signal)')\n",
    "    plt.ylabel('Source → Target')\n",
    "    plt.xlabel('Signal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Distribution des scores et des p-values\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "results_df[results_df['method']=='transfer_entropy']['score'].hist(bins=30, alpha=0.7)\n",
    "plt.title('Distribution Transfer Entropy')\n",
    "plt.xlabel('TE')\n",
    "plt.subplot(1, 2, 2)\n",
    "results_df[results_df['method']=='granger']['p_value'].hist(bins=30, alpha=0.7)\n",
    "plt.title('Distribution p-value Granger')\n",
    "plt.xlabel('p-value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau de synthèse des meilleurs signaux\n",
    "best_signals = results_df[results_df['significant']].groupby(['signal', 'method']).agg(\n",
    "    n=('score', 'count'),\n",
    "    mean_score=('score', 'mean'),\n",
    "    min_pval=('p_value', 'min')\n",
    ").sort_values('mean_score', ascending=False)\n",
    "print(\"Tableau de synthèse des signaux/méthodes les plus robustes :\")\n",
    "display(best_signals.head(10))\n",
    "\n",
    "# Export des meilleurs signaux pour l'équipe backtest\n",
    "export_cols = ['source', 'target', 'signal', 'method', 'score', 'p_value', 'lag_us', 'significant']\n",
    "best_signals_df = results_df[results_df['significant']][export_cols]\n",
    "best_signals_df.to_csv('../results/best_alpha_signals.csv', index=False)\n",
    "print(f\"Exporté {len(best_signals_df)} signaux robustes dans ../results/best_alpha_signals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signaux/méthodes jamais significatifs\n",
    "fail_signals = results_df.groupby(['signal', 'method'])['significant'].sum()\n",
    "fail_signals = fail_signals[fail_signals == 0]\n",
    "if not fail_signals.empty:\n",
    "    print(\"Signaux/méthodes sans valeur prédictive :\")\n",
    "    print(fail_signals)\n",
    "else:\n",
    "    print(\"Tous les couples signal/méthode ont au moins un cas significatif.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689e4ca",
   "metadata": {},
   "source": [
    "## 6. Conclusion et recommandations pour le backtest\n",
    "\n",
    "Ce notebook permet d'identifier objectivement les signaux et méthodes les plus robustes pour la prédiction court terme. Les résultats exportés peuvent être transmis à l'équipe backtest pour intégration dans les stratégies de trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic des colonnes synchronisées et présence des signaux attendus\n",
    "print(\"Colonnes du DataFrame synchronisé :\")\n",
    "print(list(sync_df.columns)[:30])  # Affiche les 30 premières colonnes\n",
    "\n",
    "for signal in ['mid_price', 'spread', 'imbalance', 'bid_volume_l1', 'ask_volume_l1']:\n",
    "    for sym in symbols:\n",
    "        col = f\"{sym}_{signal}\"\n",
    "        print(f\"Présence de {col} :\", col in sync_df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
